{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection techniques\n",
    "\n",
    "## Why is feature selection important?\n",
    "When working with large datasets that have many features (dimensions) you often run into whats known as the __curse of dimensionality__. The curse of dimensionality refers to the number of features (columns) we have relative to how many observations (rows) there are. At minimum you want to have 5 times as many observations as you do features. So if a dataset contains 3 features, you'd want to have a minimum of 15 observations. \n",
    "\n",
    "Say we want to classify 30 fruits into 3 categories. Apples, Oranges, or Grapes. All your given for this task is the color and size of each fruit. You could easily classify 30 fruits in not much time because you only have to account for two attributes, color and size.\n",
    "\n",
    "Now, instead of two features you're given 100 and before you can make any prediction about what tyoe of fruit you think it is, you have to consider each feature. Now the task of classifying 30 fruits isn't so quick and easy anymore now that you have to think about 100 different attributes for each fruit. \n",
    "\n",
    "If only there was a way too see which features are adding the most value and which features aren't.\n",
    "\n",
    "## Feature Selection Explained\n",
    "Feature Selection is the process of reducing the dimensionality of a dataset by eliminating features that aren't adding much value to our final prediction. For example, if we refer back to the fruit example where you're givin 100 features, theres only so much you can say about a fruit before the information given to you becomes redundant. If one of the features is the size but your also given the diameter, radius, and circumference these features aren't going to be adding much new information seeing as they're all related to the size in some sense. So by eliminating the diameter, radius, and circumference columns, it's safe to say that our prediction for what fruit we're trying to classify will still be fairly accurate.\n",
    "\n",
    "Reducing the number of features to use during a statistical analysis can possibly lead to several benefits such as:\n",
    "\n",
    "* Accuracy improvements.\n",
    "* Overfitting risk reduction.\n",
    "* Speed up in training.\n",
    "* Improved Data Visualization.\n",
    "* Increase in explainability of our model.\n",
    "\n",
    "In fact, it is statistically proven that when performing a Machine Learning task there exist an optimal number of features which should be used for every specific task. If more features are added than the ones which are strictly necessary, then our model performance will just decrease because of the added noise.\n",
    "\n",
    "![optimal number of features](https://miro.medium.com/max/1020/1*ZuFOzQawXnw_CUnVpRDLgA.png)\n",
    "\n",
    "\n",
    "There are many different methods which can be applied for Feature Selection. Some of the most important ones are:\n",
    "\n",
    "__Filter Method__ = filtering our dataset and taking only a subset of it containing all the relevant features (eg. correlation matrix using Pearson Correlation).\n",
    "\n",
    "__Wrapper Method__ = follows the same objective of the FIlter Method but uses a Machine Learning model as itâ€™s evaluation criteria (eg. Forward/Backward/Bidirectional/Recursive Feature Elimination). We feed some features to our Machine Learning model, evaluate their performance and then decide if add or remove the feature to increase accuracy. As a result, this method can be more accurate than filtering but is more computationally expensive.\n",
    "\n",
    "__Embedded Method__ = like the FIlter Method also the Embedded Method makes use of a Machine Learning model. The difference between the two methods is that the Embedded Method examines the different training iterations of our ML model and then ranks the importance of each feature based on how much each of the features contributed to the ML model training (eg. LASSO Regularization).\n",
    "\n",
    "# Practical example \n",
    "Now im going to go through the process of loading a dataset, cleaning it, finding the feature importances using the Filter and Wrapper method, then train a Logistic Regression model\n",
    "\n",
    "## Steps\n",
    "1. load neccesary libraries\n",
    "2. Read data\n",
    "3. Checking for missing values\n",
    "4. Checking for categorical data\n",
    "5. Standardize/Normalize the data (if needed)\n",
    "6. Feature importances\n",
    "7. prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful Library tips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
